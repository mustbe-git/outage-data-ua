name: Scheduled Pipeline
permissions:
  contents: write

on:
  schedule:
    # Runs every 15 minutes at :00, :15, :30, :45
    # NOTE: GitHub Actions cron uses UTC time. Adjust if you need specific local times.
    - cron: '*/15 * * * *'
  # Allows manual trigger from the Actions tab
  workflow_dispatch:
    inputs:
      region:
        description: 'Optional: process only this region id (must match a key in REGION_SOURCES_JSON)'
        required: false
        type: string

jobs:
  run:
    name: Fetch upstream HTML per region
    runs-on: ubuntu-latest
    env:
      # Expose the secret JSON map {"regionId": "url", ...}
      REGION_SOURCES_JSON: ${{ secrets.REGION_SOURCES_JSON }}
      # Optional single-region override when manually dispatching
      REGION: ${{ github.event.inputs.region }}
      # Target branch to push data updates to
      TARGET_BRANCH: main
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Cache Playwright browsers
        id: playwright-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('**/package-lock.json') }}

      - name: Ensure Playwright Chromium is installed
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: npx --yes playwright install --with-deps chromium

      - name: Fetch HTML for each region (with Playwright fallback)
        shell: bash
        run: |
          set -euo pipefail
          echo "Started at $(date -u +"%Y-%m-%dT%H:%M:%SZ") (UTC)"

          # Basic validation
          if [ -z "${REGION_SOURCES_JSON:-}" ]; then
            echo "REGION_SOURCES_JSON is empty or not set. Ensure repository secret is configured." >&2
            exit 0
          fi

          mkdir -p outputs

          # If REGION is set — pass it to the script; otherwise fetch all
          if [ -n "${REGION:-}" ]; then
            echo "[INFO] Running Playwright fetcher for region='$REGION'"
            node scripts/fetch_regions_playwright.mjs "$REGION" || true
          else
            echo "[INFO] Running Playwright fetcher for all regions"
            node scripts/fetch_regions_playwright.mjs || true
          fi

          # Always exit 0 to not fail the pipeline on partial fetch issues
          exit 0

      - name: Parse DisconSchedule.fact into JSON per region (skip failures)
        shell: bash
        run: |
          set -euo pipefail
          echo "[INFO] Parsing outputs/*.html into data/*.json via Node batch parser"
          node scripts/batch_parse.mjs

      - name: Determine changed data files
        id: changes
        shell: bash
        run: |
          set -euo pipefail
          
          # Check if templates or scripts changed
          # Since we have fetch-depth: 1, we can't easily check history. 
          # But we can check if there are unstaged changes in data/ AFTER parsing.
          # Wait, the user wants to optimize rendering time.
          # If data/*.json changed compared to what's in the repo, we render those.
          # If templates/scripts changed in the repo (pushed), we should render ALL.
          # But with fetch-depth: 1, we don't know what changed in the push unless we fetch more or use the API.
          # Actually, for a scheduled job, we are running on HEAD.
          # The "smart" part is mostly about: did the fetch/parse step update any JSON files?
          # If yes, render those.
          # If no, do nothing?
          # BUT, if I pushed a change to a template, I want everything re-rendered.
          # How to detect that?
          # Maybe we can assume that if it's a 'schedule' event, we only care about data changes.
          # If it's a 'push' event (or workflow_dispatch with code changes), we might want full render.
          # Let's stick to the data changes logic first, as that's the 99% case (every 15 mins).
          
          # We need to see which files in data/ are modified or untracked
          changed_files=$(git status --porcelain data/ | grep '\.json$' | cut -c4- || true)
          
          # Also check if we forced a specific region via input
          if [ -n "${REGION:-}" ]; then
             echo "forced_region=${REGION}" >> $GITHUB_OUTPUT
             echo "files=" >> $GITHUB_OUTPUT
             echo "run_all=false" >> $GITHUB_OUTPUT
             exit 0
          fi

          if [ -z "$changed_files" ]; then
            echo "[INFO] No data changes detected."
            echo "files=" >> $GITHUB_OUTPUT
            echo "run_all=false" >> $GITHUB_OUTPUT
          else
            # Join with commas
            files_csv=$(echo "$changed_files" | tr '\n' ',' | sed 's/,$//')
            echo "[INFO] Detected changes in: $files_csv"
            echo "files=$files_csv" >> $GITHUB_OUTPUT
            echo "run_all=false" >> $GITHUB_OUTPUT
          fi
          
          # TODO: If we want to detect template changes, we'd need to check the commit diff.
          # For now, let's assume scheduled runs only care about data.
          # If you manually trigger or push code, you might want to force run all.
          # We can add an input for that or just rely on the fact that code pushes usually happen rarely.

      - name: Generate schedule images from JSON
        shell: bash
        env:
          CHANGED_FILES: ${{ steps.changes.outputs.files }}
          FORCED_REGION: ${{ steps.changes.outputs.forced_region }}
        run: |
          set -euo pipefail
          
          if [ -n "${FORCED_REGION:-}" ]; then
            echo "[INFO] Rendering images only for region='${FORCED_REGION}'"
            node scripts/batch_render.mjs --region "${FORCED_REGION}"
            exit 0
          fi
          
          if [ -n "${CHANGED_FILES:-}" ]; then
            echo "[INFO] Rendering images for changed files: ${CHANGED_FILES}"
            node scripts/batch_render.mjs --files "${CHANGED_FILES}"
            exit 0
          fi
          
          echo "[INFO] No data changes and no forced region. Skipping render."

      - name: Commit and push data/images updates (if any)
        if: github.ref_name == env.TARGET_BRANCH
        shell: bash
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          echo "[INFO] Preparing to commit changes in data/ and images/ to ${TARGET_BRANCH}"

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

          # Show current status under data/ and images/
          echo "[INFO] Changed files under data/ and images/ (working tree):"
          git status --porcelain data/ images/ || true

          # Stage JSON files and any rendered images
          git add data/*.json 2>/dev/null || true
          git add images/** 2>/dev/null || true

          # If nothing staged, skip
          if git diff --cached --quiet; then
            echo "[INFO] No changes detected under data/ or images/ — skipping commit"
            exit 0
          fi

          echo "[INFO] Staged files:"
          git diff --name-only --cached

          # For shallow clone, we can't easily rebase if we don't have history.
          # But we are on the tip of the branch (usually).
          # If we need to pull, we might need to fetch more depth or just pull with strategy.
          # 'git pull --rebase' on shallow clone might fail if the base is missing.
          # Safe bet: fetch origin target branch with depth 1? Or just try to commit and push.
          # If push fails, we need to pull.
          
          msg_region=${REGION:-auto}
          commit_msg="data/images: update schedules and rendered images (region=${msg_region}) — run #${GITHUB_RUN_ID}"
          git commit -m "$commit_msg"

          # Retry push up to 3 times
          for attempt in 1 2 3; do
            if git push origin HEAD:"$TARGET_BRANCH"; then
              echo "[OK] Pushed data/images updates to $TARGET_BRANCH"
              exit 0
            fi
            echo "[WARN] Push failed (attempt $attempt). Fetching and rebasing..."
            # Unshallow or fetch enough to rebase?
            # Actually, 'git pull --rebase' works if we have the upstream commit.
            # Let's try fetching the branch tip first.
            git fetch origin "$TARGET_BRANCH" --depth=1
            git rebase "origin/$TARGET_BRANCH" || true
            sleep $((attempt*2))
          done

          echo "[ERROR] Failed to push changes after retries" >&2
          exit 1

      - name: Upload data JSON as artifact (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: data-json
          path: |
            data/*.json
          if-no-files-found: warn
